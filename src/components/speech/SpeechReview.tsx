import { useState, useRef, useEffect } from "react";
import { AiOutlineLineChart, AiOutlineCaretRight } from "react-icons/ai";
import { BsPauseFill, BsFillMicFill } from "react-icons/bs";
import { BiPlay, BiStop } from "react-icons/bi";
import { IoCheckboxOutline } from "react-icons/io5";
import { GiChart } from "react-icons/gi";
import { RiSpeakLine } from "react-icons/ri";
import { AiOutlineLoading3Quarters } from "react-icons/ai";
import { SpeechReviewResponseData } from "@/types/speech";
import toast from "react-hot-toast";
import SpeechStatusModal, {
	SpeechStatus,
} from "@/components/modals/SpeechStatusModal";
import { api } from "@/utils/api";
import AnimatedButton from "@/components/common/AnimatedButton";
import { useSaveSpeechNotes } from "@/hooks/speech/useSaveSpeechNotes";
import { useUpdateSpeechStatus } from "@/hooks/speech/useUpdateSpeechStatus";
import { useSpeechSentences } from "@/hooks/speech";
import { useUpdatePhraseCount } from "@/hooks/phrase/useUpdatePhraseCount";
import { useRecordSpeechPractice } from "@/hooks/speech";
import SpeakPractice from "@/components/speak/SpeakPractice";
import { useTextToSpeech } from "@/hooks/ui/useTextToSpeech";

interface SpeechReviewProps {
	speech: NonNullable<SpeechReviewResponseData["speech"]>;
	pendingCount: number;
	setPendingCount: (count: number) => void;
	viewMode: ViewMode;
	setViewMode: (mode: ViewMode) => void;
	onRefetchSpeechById: () => void;
}

type ViewMode = "review" | "practice";

export default function SpeechReview({
	speech,
	pendingCount,
	setPendingCount,
	viewMode,
	setViewMode,
	onRefetchSpeechById,
}: SpeechReviewProps) {
	const [currentPhraseIndex, setCurrentPhraseIndex] = useState(0);
	const [activeTab, setActiveTab] = useState<"Script" | "Feedback" | "Note">(
		"Script",
	);
	const [isPlaying, setIsPlaying] = useState(false);
	const audioRef = useRef<HTMLAudioElement | null>(null);
	const [isAudioLoading, setIsAudioLoading] = useState(false);
	const [audioDebugInfo, setAudioDebugInfo] = useState<string[]>([]);

	// React Query hooks
	const saveNotesMutation = useSaveSpeechNotes();
	const updateStatusMutation = useUpdateSpeechStatus();
	const updatePhraseCountMutation = useUpdatePhraseCount();
	const recordPracticeMutation = useRecordSpeechPractice();
	const {
		sentences,
		isLoading: isSentencesLoading,
		refetch: refetchSentences,
	} = useSpeechSentences({
		speechId: speech.id,
		enabled: false, // æœ€åˆã¯ç„¡åŠ¹åŒ–ã€ãƒœã‚¿ãƒ³æŠ¼ä¸‹æ™‚ã«å–å¾—
	});

	// Next/Finishãƒœã‚¿ãƒ³ã®ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°çŠ¶æ…‹
	const [isNextLoading, setIsNextLoading] = useState(false);
	const [isFinishLoading, setIsFinishLoading] = useState(false);

	// ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å¤‰æ›´ãƒ¢ãƒ¼ãƒ€ãƒ«ã®çŠ¶æ…‹
	const [isStatusModalOpen, setIsStatusModalOpen] = useState(false);
	const [statuses, setStatuses] = useState<SpeechStatus[]>([]);
	const [isLoadingStatuses, setIsLoadingStatuses] = useState(false);
	const [isOpeningStatusModal, setIsOpeningStatusModal] = useState(false);
	const [isStartingPractice, setIsStartingPractice] = useState(false);

	// ãƒ¦ãƒ¼ã‚¶ãƒ¼éŒ²éŸ³ç”¨ã®çŠ¶æ…‹
	const [isRecording, setIsRecording] = useState(false);
	const [recordingTime, setRecordingTime] = useState(90); // 1:30 = 90ç§’
	const [userAudioBlob, setUserAudioBlob] = useState<Blob | null>(null);
	const [isUserAudioPlaying, setIsUserAudioPlaying] = useState(false);
	const mediaRecorderRef = useRef<MediaRecorder | null>(null);
	const streamRef = useRef<MediaStream | null>(null);
	const userAudioRef = useRef<HTMLAudioElement | null>(null);
	const recordingStartTimeRef = useRef<number | null>(null);
	const timerIntervalRef = useRef<NodeJS.Timeout | null>(null);

	// ãƒãƒ¼ãƒˆã®ç·¨é›†çŠ¶æ…‹
	const [notes, setNotes] = useState(speech.notes || "");

	// TTSæ©Ÿèƒ½ã®åˆæœŸåŒ–
	const {
		isPlaying: isPlayingLearning,
		playText: playLearningText,
		stopAudio: stopLearningAudio,
	} = useTextToSpeech({
		languageCode: speech.learningLanguage.code,
	});

	// å­¦ç¿’è¨€èªã®ãƒ•ãƒ¬ãƒ¼ã‚ºå…¨æ–‡ã‚’çµåˆ
	const learningText = speech.phrases.map((p) => p.original).join(" ");

	// TTSå†ç”Ÿ/åœæ­¢ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
	const handleToggleLearningAudio = () => {
		if (isPlayingLearning) {
			stopLearningAudio();
		} else {
			playLearningText(learningText);
		}
	};

	// ã‚¿ãƒ–ã®ã‚¹ã‚¿ã‚¤ãƒ«
	const getTabStyle = (tab: "Script" | "Feedback" | "Note") => {
		const isActive = activeTab === tab;
		return `flex-1 pb-3 text-base font-semibold transition-colors ${
			isActive
				? "text-gray-900 border-b-2 border-gray-900"
				: "text-gray-500 hover:text-gray-700"
		}`;
	};

	// éŸ³å£°å†ç”Ÿ/ä¸€æ™‚åœæ­¢
	const handlePlayAudio = async () => {
		if (!speech.audioFilePath) {
			setAudioDebugInfo((prev) => [...prev, "âŒ No audio file path available"]);
			toast.error("No audio file path available");
			return;
		}

		// æ—¢å­˜ã®éŸ³å£°ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒã‚ã‚‹å ´åˆ
		if (audioRef.current) {
			if (isPlaying) {
				audioRef.current.pause();
				setIsPlaying(false);
				setAudioDebugInfo((prev) => [...prev, "â¸ï¸ Audio paused"]);
				return;
			}

			// ä¸€æ™‚åœæ­¢ä¸­ã®éŸ³å£°ã‚’å†é–‹
			try {
				await audioRef.current.play();
				setIsPlaying(true);
				setAudioDebugInfo((prev) => [...prev, "â–¶ï¸ Audio resumed"]);
			} catch (error) {
				console.error("Failed to play audio:", error);
				setAudioDebugInfo((prev) => [...prev, `âŒ Failed to resume: ${error}`]);
				toast.error("Failed to play audio");
				audioRef.current = null;
				setIsPlaying(false);
			}
			return;
		}

		// ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’ãƒªã‚»ãƒƒãƒˆ
		setAudioDebugInfo([]);

		// æ–°ã—ã„éŸ³å£°ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆï¼ˆSafariå¯¾å¿œï¼šç›´æ¥URLã‚’ä½¿ç”¨ï¼‰
		setIsAudioLoading(true);
		setAudioDebugInfo((prev) => [...prev, "ğŸ”„ Creating audio element..."]);

		try {
			// Safariå¯¾å¿œï¼šfetch/blobçµŒç”±ã‚’ã‚„ã‚ã¦ç›´æ¥URLã‚’ä½¿ç”¨
			console.log("Audio URL:", speech.audioFilePath);
			setAudioDebugInfo((prev) => [
				...prev,
				`ğŸ“‹ URL length: ${speech?.audioFilePath?.length} chars`,
			]);
			setAudioDebugInfo((prev) => [
				...prev,
				`ğŸ”— URL preview: ${speech.audioFilePath?.substring(0, 80)}...`,
			]);

			const audio = new Audio(speech.audioFilePath);
			audioRef.current = audio;

			// éŸ³é‡ã‚’æœ€å¤§ã«è¨­å®š
			audio.volume = 1.0;

			setAudioDebugInfo((prev) => [...prev, "âœ… Audio element created"]);
			setAudioDebugInfo((prev) => [
				...prev,
				`ğŸ”Š Volume set to: ${audio.volume}`,
			]);

			// ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒŠãƒ¼ã‚’è¨­å®š
			audio.onloadstart = () => {
				console.log("Audio load started");
				setAudioDebugInfo((prev) => [...prev, "ğŸ“¥ Audio loading started..."]);
			};

			audio.onloadedmetadata = () => {
				console.log("Audio metadata loaded");
				setAudioDebugInfo((prev) => [
					...prev,
					`ğŸ“Š Metadata loaded (duration: ${audio.duration}s)`,
				]);
			};

			audio.oncanplay = () => {
				console.log("Audio can play");
				setAudioDebugInfo((prev) => [...prev, "âœ… Audio ready to play!"]);
				setAudioDebugInfo((prev) => [
					...prev,
					`ğŸ“Š Current time: ${audio.currentTime}s`,
				]);
				setAudioDebugInfo((prev) => [...prev, `ğŸ”Š Volume: ${audio.volume}`]);
				setAudioDebugInfo((prev) => [...prev, `ğŸ”‡ Muted: ${audio.muted}`]);
				setAudioDebugInfo((prev) => [...prev, `â¯ï¸ Paused: ${audio.paused}`]);
			};

			audio.onended = () => {
				setIsPlaying(false);
				setAudioDebugInfo((prev) => [...prev, "ğŸ Audio ended"]);
			};

			audio.onerror = (e) => {
				console.error("Audio element error event:", e);
				console.error("Audio error details:", {
					error: audio.error,
					code: audio.error?.code,
					message: audio.error?.message,
					networkState: audio.networkState,
					readyState: audio.readyState,
				});
				setIsPlaying(false);
				setIsAudioLoading(false);

				// Safariç”¨ã®è©³ç´°ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
				const errorCode = audio.error?.code;
				const errorMessages: { [key: number]: string } = {
					1: "MEDIA_ERR_ABORTED: Audio loading was aborted",
					2: "MEDIA_ERR_NETWORK: Network error occurred",
					3: "MEDIA_ERR_DECODE: Audio decoding failed",
					4: "MEDIA_ERR_SRC_NOT_SUPPORTED: Audio format not supported or Range Request failed (Safari)",
				};

				const errorMsg = errorCode
					? `${errorMessages[errorCode] || `Unknown error (code: ${errorCode})`}`
					: `Failed to load audio`;

				setAudioDebugInfo((prev) => [...prev, `âŒ ${errorMsg}`]);
				setAudioDebugInfo((prev) => [
					...prev,
					`ğŸ“Š Network: ${audio.networkState}, Ready: ${audio.readyState}`,
				]);

				toast.error(errorMsg, { duration: 8000 });

				audioRef.current = null;
			};

			// Safariå¯¾å¿œï¼šãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³å†…ã§å³åº§ã«play()ã‚’å‘¼ã¶
			console.log("Attempting to play...");
			setAudioDebugInfo((prev) => [...prev, "â–¶ï¸ Calling play()..."]);

			const playPromise = audio.play();

			if (playPromise !== undefined) {
				playPromise
					.then(() => {
						console.log("Play promise resolved");
						setIsPlaying(true);
						setIsAudioLoading(false);
						setAudioDebugInfo((prev) => [...prev, "âœ… Audio playing!"]);

						// å†ç”ŸçŠ¶æ…‹ã‚’ç¢ºèª
						setTimeout(() => {
							if (audioRef.current) {
								setAudioDebugInfo((prev) => [
									...prev,
									`â±ï¸ Playing for: ${audioRef.current!.currentTime}s`,
								]);
								setAudioDebugInfo((prev) => [
									...prev,
									`â¯ï¸ Is paused: ${audioRef.current!.paused}`,
								]);
								setAudioDebugInfo((prev) => [
									...prev,
									`ğŸ”Š Volume: ${audioRef.current!.volume}`,
								]);
								setAudioDebugInfo((prev) => [
									...prev,
									`ğŸ”‡ Muted: ${audioRef.current!.muted}`,
								]);
							}
						}, 1000);
					})
					.catch((error) => {
						console.error("Play promise rejected:", error);
						console.error("Error name:", error.name);
						console.error("Error message:", error.message);

						// Safariç”¨ã®è©³ç´°ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
						const urlPreview = speech.audioFilePath?.substring(0, 100) || "N/A";
						setAudioDebugInfo((prev) => [
							...prev,
							`âŒ Play failed: ${error.name}`,
						]);
						setAudioDebugInfo((prev) => [...prev, `ğŸ“ ${error.message}`]);
						setAudioDebugInfo((prev) => [...prev, `ğŸ”— URL: ${urlPreview}...`]);

						const errorDetails = `Play failed: ${error.name}\n${error.message}`;
						toast.error(errorDetails, { duration: 8000 });

						audioRef.current = null;
						setIsPlaying(false);
						setIsAudioLoading(false);
					});
			} else {
				console.log("Play returned undefined (old browser)");
				setIsPlaying(true);
				setIsAudioLoading(false);
				setAudioDebugInfo((prev) => [...prev, "âœ… Audio playing (sync)"]);
			}
		} catch (error) {
			console.error("Exception in handlePlayAudio:", error);
			const errorMsg =
				error instanceof Error
					? `${error.name}: ${error.message}`
					: String(error);
			setAudioDebugInfo((prev) => [...prev, `âŒ Exception: ${errorMsg}`]);

			toast.error(errorMsg, { duration: 8000 });
			audioRef.current = null;
			setIsPlaying(false);
			setIsAudioLoading(false);
		}
	}; // ãƒ¦ãƒ¼ã‚¶ãƒ¼éŒ²éŸ³ã®é–‹å§‹
	const startUserRecording = async () => {
		try {
			const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
			streamRef.current = stream;

			const mediaRecorder = new MediaRecorder(stream);
			mediaRecorderRef.current = mediaRecorder;

			const chunks: Blob[] = [];

			mediaRecorder.ondataavailable = (e) => {
				if (e.data.size > 0) {
					chunks.push(e.data);
				}
			};

			mediaRecorder.onstop = () => {
				// éŒ²éŸ³æ™‚é–“ã‚’ãƒã‚§ãƒƒã‚¯
				const recordingDuration = recordingStartTimeRef.current
					? (Date.now() - recordingStartTimeRef.current) / 1000
					: 0;

				// 10ç§’ä»¥ä¸Šã®å ´åˆã®ã¿Blobã‚’ä¿å­˜
				if (recordingDuration >= 3) {
					const blob = new Blob(chunks, { type: "audio/webm" });
					setUserAudioBlob(blob);
				}

				// ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’åœæ­¢
				if (streamRef.current) {
					streamRef.current.getTracks().forEach((track) => track.stop());
					streamRef.current = null;
				}

				// ä½¿ç”¨æ¸ˆã¿ãªã®ã§ã‚¯ãƒªã‚¢
				recordingStartTimeRef.current = null;
			};

			mediaRecorder.start();
			setIsRecording(true);
			recordingStartTimeRef.current = Date.now();
			setRecordingTime(90);

			// ã‚¿ã‚¤ãƒãƒ¼é–‹å§‹
			timerIntervalRef.current = setInterval(() => {
				setRecordingTime((prev) => {
					if (prev <= 1) {
						// æ™‚é–“åˆ‡ã‚Œã§è‡ªå‹•åœæ­¢
						return 0;
					}
					return prev - 1;
				});
			}, 1000);
		} catch (error) {
			console.error("Failed to start recording:", error);
			alert("ãƒã‚¤ã‚¯ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ãŒè¨±å¯ã•ã‚Œã¦ã„ã¾ã›ã‚“");
		}
	};

	// ãƒ¦ãƒ¼ã‚¶ãƒ¼éŒ²éŸ³ã®åœæ­¢
	const stopUserRecording = () => {
		if (mediaRecorderRef.current && isRecording) {
			// ã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚¯ãƒªã‚¢
			if (timerIntervalRef.current) {
				clearInterval(timerIntervalRef.current);
				timerIntervalRef.current = null;
			}

			// éŒ²éŸ³æ™‚é–“ã‚’ãƒã‚§ãƒƒã‚¯
			const recordingDuration = recordingStartTimeRef.current
				? (Date.now() - recordingStartTimeRef.current) / 1000
				: 0;

			if (recordingDuration < 3) {
				// éŒ²éŸ³æ™‚é–“ãŒ10ç§’æœªæº€ã®å ´åˆ
				toast.error("éŒ²éŸ³æ™‚é–“ãŒçŸ­ã™ãã¾ã™");
				// ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’åœæ­¢
				if (streamRef.current) {
					streamRef.current.getTracks().forEach((track) => track.stop());
					streamRef.current = null;
				}
				// MediaRecorderã‚’åœæ­¢ï¼ˆondatavailableã¯ç™ºç«ã™ã‚‹ãŒä¿å­˜ã—ãªã„ï¼‰
				mediaRecorderRef.current.stop();
				setIsRecording(false);
				recordingStartTimeRef.current = null;
				setRecordingTime(90); // ã‚¿ã‚¤ãƒãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆ
				// userAudioBlobã¯è¨­å®šã—ãªã„ã®ã§éŒ²éŸ³ãƒœã‚¿ãƒ³ã®è¡¨ç¤ºã®ã¾ã¾
				return;
			}

			// 10ç§’ä»¥ä¸Šã®å ´åˆã¯é€šå¸¸é€šã‚Šåœæ­¢
			mediaRecorderRef.current.stop();
			setIsRecording(false);
			// recordingStartTimeRefã¯onstopã§ä½¿ã†ã®ã§ã“ã“ã§ã¯ã‚¯ãƒªã‚¢ã—ãªã„
		}
	};

	// ãƒ¦ãƒ¼ã‚¶ãƒ¼éŒ²éŸ³ã®å†ç”Ÿ/ä¸€æ™‚åœæ­¢
	const handleUserAudioPlay = () => {
		if (!userAudioBlob) return;

		if (!userAudioRef.current) {
			const audio = new Audio(URL.createObjectURL(userAudioBlob));
			userAudioRef.current = audio;

			audio.onended = () => {
				setIsUserAudioPlaying(false);
				// ãƒˆãƒ¼ã‚¹ãƒˆã‚’è¡¨ç¤º
				toast.success("Review completed!");
				// Speechç·´ç¿’è¨˜éŒ²APIã‚’å‘¼ã³å‡ºã™
				recordPracticeMutation.mutate(
					{ speechId: speech.id },
					{
						onSuccess: () => {
							// ç·´ç¿’å›æ•°ã‚’æ›´æ–°å¾Œã€Speechã®IDã‚’ä½¿ã£ã¦å†å–å¾—
							onRefetchSpeechById();
						},
						onError: (error) => {
							console.error("Failed to record practice:", error);
							toast.error("Failed to record practice");
						},
					},
				);
				// éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆ
				setTimeout(() => {
					if (userAudioRef.current) {
						userAudioRef.current = null;
					}
					setUserAudioBlob(null);
					setRecordingTime(90);
				}, 500);
			};
			audio.onerror = () => {
				setIsUserAudioPlaying(false);
				console.error("Failed to load user audio");
			};

			// æ–°ã—ãä½œæˆã—ãŸéŸ³å£°ã‚’å†ç”Ÿ
			audio.play().catch((error) => {
				console.error("Failed to play user audio:", error);
				setIsUserAudioPlaying(false);
			});
			setIsUserAudioPlaying(true);
			return;
		}

		if (isUserAudioPlaying) {
			userAudioRef.current.pause();
			setIsUserAudioPlaying(false);
		} else {
			userAudioRef.current.play().catch((error) => {
				console.error("Failed to play user audio:", error);
				setIsUserAudioPlaying(false);
			});
			setIsUserAudioPlaying(true);
		}
	};

	// ãƒ¦ãƒ¼ã‚¶ãƒ¼éŒ²éŸ³ãƒœã‚¿ãƒ³ã®ã‚¯ãƒªãƒƒã‚¯å‡¦ç†
	const handleUserRecordButtonClick = () => {
		if (isRecording) {
			stopUserRecording();
		} else if (userAudioBlob) {
			handleUserAudioPlay();
		} else {
			startUserRecording();
		}
	};

	// ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
	useEffect(() => {
		return () => {
			if (audioRef.current) {
				audioRef.current.pause();
				audioRef.current = null;
			}
			if (userAudioRef.current) {
				userAudioRef.current.pause();
				userAudioRef.current = null;
			}
			if (streamRef.current) {
				streamRef.current.getTracks().forEach((track) => track.stop());
			}
			if (timerIntervalRef.current) {
				clearInterval(timerIntervalRef.current);
			}
		};
	}, []);

	// textareaã®é«˜ã•ã‚’åˆæœŸåŒ–æ™‚ã¨å†…å®¹å¤‰æ›´æ™‚ã«èª¿æ•´
	useEffect(() => {
		setTimeout(() => {
			const textareas =
				document.querySelectorAll<HTMLTextAreaElement>("textarea");
			textareas.forEach((textarea) => {
				textarea.style.height = "auto";
				textarea.style.height = `${textarea.scrollHeight}px`;
			});
		}, 0);
	}, [notes]);

	// ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ãƒ¢ãƒ¼ãƒ‰ã®é–‹å§‹
	const handleStartPractice = async () => {
		setIsStartingPractice(true);
		try {
			// ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
			await refetchSentences();
			setViewMode("practice");
			setCurrentPhraseIndex(0);
		} finally {
			setIsStartingPractice(false);
		}
	};

	// ã‚«ã‚¦ãƒ³ãƒˆãƒœã‚¿ãƒ³ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
	const handleCountSpeech = () => {
		const currentSentence = sentences[currentPhraseIndex];
		if (!currentSentence) return;

		const newPendingCount = pendingCount + 1;
		const newDailyCount =
			(currentSentence.dailySpeakCount || 0) + newPendingCount;

		// 1æ—¥ã®ã‚«ã‚¦ãƒ³ãƒˆä¸Šé™ãƒã‚§ãƒƒã‚¯ï¼ˆ100å›ï¼‰
		if (newDailyCount > 100) {
			toast.error("Daily limit reached (100 times)", {
				duration: 4000,
			});
			return;
		}

		setPendingCount(newPendingCount);

		if (newDailyCount === 100) {
			toast.error("Daily limit reached (100 times)", {
				duration: 4000,
			});
		}
	};

	// æ¬¡ã®ãƒ•ãƒ¬ãƒ¼ã‚ºã¸é€²ã‚€
	const handleNextPhrase = async () => {
		setIsNextLoading(true);
		try {
			const currentSentence = sentences[currentPhraseIndex];

			// ãƒšãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã‚«ã‚¦ãƒ³ãƒˆãŒã‚ã‚‹å ´åˆã¯é€ä¿¡
			if (pendingCount > 0 && currentSentence) {
				try {
					await updatePhraseCountMutation.mutateAsync({
						phraseId: currentSentence.id,
						count: pendingCount,
					});
					setPendingCount(0);
					// ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å†å–å¾—ã—ã¦æ›´æ–°ã•ã‚ŒãŸã‚«ã‚¦ãƒ³ãƒˆã‚’åæ˜ 
					await refetchSentences();
				} catch {
					toast.error("Failed to update count");
					return;
				}
			} else if (currentSentence) {
				// ã‚«ã‚¦ãƒ³ãƒˆãŒ0ã§ã‚‚session_spokenã‚’trueã«è¨­å®š
				try {
					await updatePhraseCountMutation.mutateAsync({
						phraseId: currentSentence.id,
						count: 0,
					});
				} catch {
					// session_spokenè¨­å®šã‚¨ãƒ©ãƒ¼ã¯æ¬¡ã®ãƒ•ãƒ¬ãƒ¼ã‚ºå–å¾—ã‚’é˜»å®³ã—ãªã„
				}
			}

			if (currentPhraseIndex < sentences.length - 1) {
				setCurrentPhraseIndex(currentPhraseIndex + 1);
			}
		} finally {
			setIsNextLoading(false);
		}
	};

	// ãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹å®Œäº†
	const handleFinishPractice = async () => {
		setIsFinishLoading(true);
		try {
			const currentSentence = sentences[currentPhraseIndex];

			// ãƒšãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã‚«ã‚¦ãƒ³ãƒˆãŒã‚ã‚‹å ´åˆã¯é€ä¿¡
			if (pendingCount > 0 && currentSentence) {
				try {
					await updatePhraseCountMutation.mutateAsync({
						phraseId: currentSentence.id,
						count: pendingCount,
					});
					setPendingCount(0);
				} catch {
					toast.error("Failed to update count");
					return;
				}
			} else if (currentSentence) {
				// ã‚«ã‚¦ãƒ³ãƒˆãŒ0ã§ã‚‚session_spokenã‚’trueã«è¨­å®š
				try {
					await updatePhraseCountMutation.mutateAsync({
						phraseId: currentSentence.id,
						count: 0,
					});
				} catch {
					// ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
				}
			}

			// Reviewç”»é¢ã«æˆ»ã‚‹
			setViewMode("review");
			setCurrentPhraseIndex(0);
		} finally {
			setIsFinishLoading(false);
		}
	};

	// ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ä¸€è¦§ã‚’å–å¾—
	const fetchStatuses = async () => {
		setIsLoadingStatuses(true);
		try {
			const data = await api.get<{ statuses: SpeechStatus[] }>(
				"/api/speech/statuses",
			);
			setStatuses(data.statuses);
		} catch (error) {
			console.error("Failed to fetch statuses:", error);
			toast.error("Failed to load statuses");
		} finally {
			setIsLoadingStatuses(false);
		}
	};

	// ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å¤‰æ›´å‡¦ç†
	const handleStatusChange = async (statusId: string) => {
		await updateStatusMutation.mutateAsync({
			speechId: speech.id,
			statusId,
		});
	};

	// ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ¢ãƒ¼ãƒ€ãƒ«ã‚’é–‹ã
	const handleOpenStatusModal = async () => {
		setIsOpeningStatusModal(true);
		try {
			if (statuses.length === 0) {
				await fetchStatuses();
			}
			setIsStatusModalOpen(true);
		} finally {
			setIsOpeningStatusModal(false);
		}
	};

	// ãƒãƒ¼ãƒˆä¿å­˜å‡¦ç†
	const handleSaveNotes = async () => {
		await saveNotesMutation.mutateAsync({
			speechId: speech.id,
			notes,
		});
	};

	// ãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ
	if (viewMode === "practice") {
		const currentSentence = sentences[currentPhraseIndex] || null;
		const isLastSentence = currentPhraseIndex >= sentences.length - 1;

		return (
			<SpeakPractice
				phrase={currentSentence}
				onCount={handleCountSpeech}
				onNext={handleNextPhrase}
				onFinish={handleFinishPractice}
				todayCount={(currentSentence?.dailySpeakCount ?? 0) + pendingCount}
				totalCount={(currentSentence?.totalSpeakCount ?? 0) + pendingCount}
				isLoading={isSentencesLoading}
				isNextLoading={isNextLoading}
				isHideNext={isLastSentence}
				isFinishing={isFinishLoading}
				isCountDisabled={false}
				learningLanguage={speech.learningLanguage.code}
				onExplanation={
					currentSentence?.explanation
						? () => {
								toast.success(currentSentence.explanation || "");
							}
						: undefined
				}
			/>
		);
	}

	// Reviewç”»é¢ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
	return (
		<div className="max-w-4xl mx-auto">
			{/* Header with checkmark, practice count, and fullscreen */}
			<div className="flex items-center justify-between mb-4">
				<h1 className="text-2xl font-bold text-gray-900">Review Speech</h1>
				<div className="flex items-center gap-4">
					{/* Practice count */}
					<div className="flex items-center gap-2">
						<IoCheckboxOutline className="w-5 h-5 text-gray-700" />
						<span className="text-sm font-medium">{speech.practiceCount}</span>
					</div>
					{/* Status */}
					<div className="flex items-center gap-2">
						<GiChart className="w-5 h-5 text-gray-700" />
						<span className="text-sm font-medium">{speech.status.name}</span>
					</div>
				</div>
			</div>

			{/* Title */}
			<div className="mb-6">
				<h2 className="text-xl font-semibold text-gray-900 mb-2">Title</h2>
				<div className="text-base text-gray-900">{speech.title}</div>
			</div>
			{/* Tabs */}
			<div className="flex border-b border-gray-300 mb-6">
				<button
					type="button"
					className={getTabStyle("Script")}
					onClick={() => setActiveTab("Script")}
				>
					Script
				</button>
				<button
					type="button"
					className={getTabStyle("Feedback")}
					onClick={() => setActiveTab("Feedback")}
				>
					Feedback
				</button>
				<button
					type="button"
					className={getTabStyle("Note")}
					onClick={() => setActiveTab("Note")}
				>
					Note
				</button>
			</div>
			{/* Tab Content */}
			<div className="mb-6">
				{activeTab === "Script" && (
					<div className="space-y-6">
						<p className="text-sm text-gray-700 mb-4">
							æµæš¢ã«è©±ã›ã‚‹ã‚ˆã†ã«ãªã‚‹ã¾ã§ç·´ç¿’ã—ã¾ã—ã‚‡ã†ã€‚
						</p>
						<div>
							<div className="flex items-center justify-between mb-3">
								<h3 className="text-lg font-semibold text-gray-900">
									{speech.learningLanguage.name}
								</h3>
								<button
									type="button"
									onClick={handleToggleLearningAudio}
									className="w-9 h-9 rounded-full border border-gray-300 flex items-center justify-center hover:bg-gray-50 transition-colors"
								>
									{isPlayingLearning ? (
										<BiStop size={18} className="text-gray-600" />
									) : (
										<div className="w-0 h-0 border-t-[5px] border-t-transparent border-l-[7px] border-l-gray-600 border-b-[5px] border-b-transparent ml-1" />
									)}
								</button>
							</div>
							<div className="border border-gray-300 rounded-lg p-4 bg-white space-y-4">
								{speech.phrases.map((phrase) => (
									<p key={phrase.id} className="text-sm text-gray-900">
										{phrase.original}
									</p>
								))}
							</div>
						</div>
						<div>
							<h3 className="text-lg font-semibold text-gray-900 mb-3">
								{speech.nativeLanguage.name}
							</h3>
							<div className="border border-gray-300 rounded-lg p-4 bg-white space-y-4">
								{speech.phrases.map((phrase) => (
									<p key={phrase.id} className="text-sm text-gray-900">
										{phrase.translation}
									</p>
								))}
							</div>
						</div>
					</div>
				)}
				{activeTab === "Feedback" && (
					<div className="space-y-6">
						<p className="text-sm text-gray-700 mb-4">
							åˆå›ã‚¹ãƒ”ãƒ¼ãƒã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ç¢ºèªã—ã¾ã—ã‚‡ã†ã€‚
						</p>
						{/* Your Speech Section */}
						<div>
							<div className="flex items-center justify-between mb-3">
								<h3 className="text-xl font-semibold text-gray-900">
									Your Speech
								</h3>
								{speech.audioFilePath && (
									<button
										type="button"
										onClick={handlePlayAudio}
										disabled={isAudioLoading}
										className="w-9 h-9 rounded-full border border-gray-300 flex items-center justify-center hover:bg-gray-50 transition-colors disabled:opacity-50 disabled:cursor-not-allowed"
									>
										{isAudioLoading ? (
											<AiOutlineLoading3Quarters
												size={18}
												className="text-gray-600 animate-spin"
											/>
										) : isPlaying ? (
											<BsPauseFill size={18} className="text-gray-600" />
										) : (
											<div className="w-0 h-0 border-t-[5px] border-t-transparent border-l-[7px] border-l-gray-600 border-b-[5px] border-b-transparent ml-1" />
										)}
									</button>
								)}
							</div>

							{/* ãƒ‡ãƒãƒƒã‚°æƒ…å ±è¡¨ç¤º */}
							{audioDebugInfo.length > 0 && (
								<div className="mb-4 p-3 bg-gray-100 border border-gray-300 rounded-lg">
									<div className="text-xs font-mono text-gray-700 space-y-1">
										{audioDebugInfo.map((info, index) => (
											<div key={index}>{info}</div>
										))}
									</div>
								</div>
							)}

							<div className="border border-gray-300 rounded-lg p-4 bg-white">
								{speech.firstSpeechText ? (
									<p className="text-sm text-gray-900 whitespace-pre-wrap">
										{speech.firstSpeechText}
									</p>
								) : (
									<p className="text-gray-500 text-center py-4">
										No transcript available
									</p>
								)}
							</div>
						</div>{" "}
						{/* Feedback Section */}
						<div>
							<h3 className="text-xl font-semibold text-gray-900 mb-2">
								Feedback
							</h3>
							<div className="space-y-6">
								{speech.feedbacks.length > 0 ? (
									speech.feedbacks.map((feedback) => (
										<div key={feedback.id}>
											{/* Header */}
											<div className="flex items-center mb-3">
												<AiOutlineCaretRight
													size={16}
													className="text-gray-600 mr-1"
												/>
												<span className="font-medium text-gray-900 text-md">
													{feedback.category}
												</span>
											</div>

											{/* Content */}
											<div className="p-3 bg-gray-50 border border-gray-200 rounded-md">
												<p className="text-sm text-gray-700 leading-relaxed whitespace-pre-wrap">
													{feedback.content}
												</p>
											</div>
										</div>
									))
								) : (
									<div className="text-center py-8 text-gray-500">
										No feedback available
									</div>
								)}
							</div>
						</div>
					</div>
				)}{" "}
				{activeTab === "Note" && (
					<div className="space-y-4">
						<p className="text-sm text-gray-700">
							æ°—ã¥ã„ãŸã“ã¨ã‚’è‡ªç”±ã«ãƒ¡ãƒ¢ã—ã¾ã—ã‚‡ã†ã€‚
						</p>
						<textarea
							value={notes}
							onChange={(e) => setNotes(e.target.value)}
							onInput={(e) => {
								const target = e.target as HTMLTextAreaElement;
								target.style.height = "auto";
								target.style.height = `${target.scrollHeight}px`;
							}}
							placeholder="ãƒ¡ãƒ¢ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"
							className="w-full min-h-[350px] p-4 border border-gray-300 rounded-lg focus:outline-none focus:ring-2 focus:ring-gray-400 text-base text-gray-900 resize-none bg-white overflow-hidden"
						/>
						<AnimatedButton
							onClick={handleSaveNotes}
							disabled={saveNotesMutation.isPending}
							isLoading={saveNotesMutation.isPending}
							size="lg"
						>
							Save
						</AnimatedButton>
					</div>
				)}
			</div>
			{/* Audio Player */}
			{activeTab === "Script" && (
				<div className="pt-2">
					{/* ã‚¿ã‚¤ãƒãƒ¼è¡¨ç¤º */}
					<div className="text-center mb-6">
						<div className="text-2xl font-bold text-gray-900">
							{Math.floor(recordingTime / 60)}:
							{String(recordingTime % 60).padStart(2, "0")}
						</div>
					</div>

					{/* ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒœã‚¿ãƒ³ */}
					<div className="flex justify-center items-center gap-8">
						{/* Analytics button (å·¦å´) */}
						<button
							type="button"
							onClick={handleOpenStatusModal}
							disabled={isOpeningStatusModal}
							className="w-14 h-14 rounded-full border-2 border-gray-300 flex items-center justify-center text-gray-600 hover:bg-gray-50 transition-colors disabled:opacity-50 disabled:cursor-not-allowed"
						>
							{isOpeningStatusModal ? (
								<AiOutlineLoading3Quarters size={24} className="animate-spin" />
							) : (
								<AiOutlineLineChart size={24} />
							)}
						</button>

						{/* User Recording button (ä¸­å¤®) - ãƒ¦ãƒ¼ã‚¶ãƒ¼éŒ²éŸ³ */}
						<button
							type="button"
							onClick={handleUserRecordButtonClick}
							className={`w-20 h-20 rounded-full flex items-center justify-center text-white transition-colors shadow-lg ${
								isRecording ? "animate-pulse" : ""
							}`}
							style={{ backgroundColor: "#616161" }}
							onMouseEnter={(e) => {
								e.currentTarget.style.backgroundColor = "#525252";
							}}
							onMouseLeave={(e) => {
								e.currentTarget.style.backgroundColor = "#616161";
							}}
						>
							{isRecording ? (
								<BiStop size={40} />
							) : userAudioBlob ? (
								isUserAudioPlaying ? (
									<BsPauseFill size={32} />
								) : (
									<BiPlay size={40} />
								)
							) : (
								<BsFillMicFill size={32} />
							)}
						</button>

						{/* Speaker button (å³å´) - speech.audioFilePathã®å†ç”Ÿ */}
						<button
							type="button"
							onClick={handleStartPractice}
							disabled={isStartingPractice}
							className="w-14 h-14 rounded-full border-2 border-gray-300 flex items-center justify-center text-gray-600 hover:bg-gray-50 transition-colors disabled:opacity-50 disabled:cursor-not-allowed"
						>
							{isStartingPractice ? (
								<AiOutlineLoading3Quarters size={24} className="animate-spin" />
							) : (
								<RiSpeakLine size={24} />
							)}
						</button>
					</div>
				</div>
			)}

			{/* ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å¤‰æ›´ãƒ¢ãƒ¼ãƒ€ãƒ« */}
			<SpeechStatusModal
				isOpen={isStatusModalOpen}
				onClose={() => setIsStatusModalOpen(false)}
				statuses={statuses}
				currentStatusId={speech.status.id}
				onStatusChange={handleStatusChange}
				isLoading={isLoadingStatuses}
			/>
		</div>
	);
}
